{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training program on AI/ML\n",
        "\n",
        "# Demo for representation learning\n",
        "\n",
        "### Linguistic Analogy Evaluation Using FAIR's FastText Embeddings Trained on Wikipedia and News Corpora [Link](https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip)\n"
      ],
      "metadata": {
        "id": "yNac5QS5pBLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KNeD5h0UCf7C",
        "outputId": "fc0a1464-eaab-4f66-e3cf-2b09a3094842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "8BFavFNlICSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3011712c-367f-4fbf-a2d9-99028e0d2364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install emoji"
      ],
      "metadata": {
        "id": "u3q35_jMN-Ar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c351e2-f81e-4e0d-c373-ccdb002f8fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText"
      ],
      "metadata": {
        "id": "n3Fh5e8Sp0eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim, logging, os\n",
        "from gensim import corpora\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import time"
      ],
      "metadata": {
        "id": "6OEtg7J6p0T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets/wiki-news-300d-1M.vec', binary=False)"
      ],
      "metadata": {
        "id": "v32AYn_7pAyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word similarities"
      ],
      "metadata": {
        "id": "P-iBwjucsP7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sims = wv_model.most_similar('intelligence', topn=10)\n",
        "for word, score in sims:\n",
        "    print(f\"{word}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "Wk5BJC-NsKHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analogy"
      ],
      "metadata": {
        "id": "QvRrhcVxsTDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = wv_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)\n",
        "print(result) # Expected: [('queen', ...)]"
      ],
      "metadata": {
        "id": "o9Ip2XGupAh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvXtWzu7CTSc"
      },
      "source": [
        "## Dataset loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHNs7IjbCTSc"
      },
      "outputs": [],
      "source": [
        "f=open('/content/drive/MyDrive/Datasets/Demo-notebook/sentiment.txt','r')\n",
        "sentences=f.readlines()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqrGDr5NCTSd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
        "\n",
        "def text_cleaner(text):\n",
        "    text=remove_link(text.lower())\n",
        "    long_words=[]\n",
        "    for i in text.split():\n",
        "        if i not in stopwords:\n",
        "            long_words.append(i)\n",
        "    return long_words\n",
        "\n",
        "def remove_link(text):\n",
        "    regex = r'https?://[^\\s<>)\"‘’]+'\n",
        "    match = re.sub(regex,' ', text)\n",
        "    regex = r'https?:|urls?|[/\\:,-.\"\\'?!;…]+'\n",
        "    tweet = re.sub(regex,' ', match)\n",
        "    tweet = re.sub(\"[^a-zA-Z_]\", \" \", tweet)\n",
        "    tweet = re.sub(\"[ ]+\", \" \", tweet)\n",
        "    return tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBHZEtUCCTSd"
      },
      "outputs": [],
      "source": [
        "vocabulary=[]\n",
        "X=[]\n",
        "y=[]\n",
        "for line in sentences:\n",
        "\ttexts=line.strip().lower().split('\\t')\n",
        "\ttokens=text_cleaner(texts[0])\n",
        "\ty.append(texts[1])\n",
        "\tX.append(tokens)\n",
        "\tfor word in tokens:\n",
        "\t\tvocabulary.append(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj0OIt-_CTSe"
      },
      "source": [
        "## Conversion of text to vector space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7y8O7hQCTSe"
      },
      "outputs": [],
      "source": [
        "def t2v(tokens,vocabulary):\n",
        "\tvect=[]\n",
        "\tfor feature in vocabulary:\n",
        "\t\tif feature in tokens:\n",
        "\t\t\tvect.append(1)\n",
        "\t\telse:\n",
        "\t\t\tvect.append(0)\n",
        "\treturn vect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYi_1vlLCTSf"
      },
      "outputs": [],
      "source": [
        "vector=[]\n",
        "for tokens in X:\n",
        "\tvect=t2v(tokens,vocabulary)\n",
        "\tvector.append(vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe22ofvuCTSf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "vector=np.asarray(vector)\n",
        "y=np.asarray(y)\n",
        "print(\"Number of samples,Number of features\")\n",
        "vector.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmR8ZnSbCTSg"
      },
      "source": [
        "# Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Lj6nJHCTSh"
      },
      "source": [
        "### Mutual Information based feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39vtDs0jCTSh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "X_new = mutual_info_classif(vector,y)\n",
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGej8_gZCTSh"
      },
      "outputs": [],
      "source": [
        "N=len(vocabulary)\n",
        "avg=0\n",
        "for mi in X_new:\n",
        "    avg+=mi\n",
        "avg=avg/N\n",
        "feat_selection=[]\n",
        "for i,mi in enumerate(X_new):\n",
        "    if mi>avg:\n",
        "        feat_selection.append(vocabulary[i])\n",
        "len(feat_selection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1wJ313ECTSi"
      },
      "outputs": [],
      "source": [
        "feat_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFsGQh0jCTSi"
      },
      "outputs": [],
      "source": [
        "vector_feat=[]\n",
        "for tokens in X:\n",
        "\tvect=t2v(tokens,feat_selection)\n",
        "\tvector_feat.append(vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HinF2QViCTSi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "vector_feat=np.asarray(vector_feat)\n",
        "vector_feat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-3XRJ5JCTSi"
      },
      "outputs": [],
      "source": [
        "test_ip=\"I love watching netflix original movies\"\n",
        "tokens=text_cleaner(test_ip)\n",
        "vect_pos=t2v(tokens,feat_selection)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-j6XGu-CTSi"
      },
      "outputs": [],
      "source": [
        "test_ip=\"I hate watching netflix original movies\"\n",
        "tokens=text_cleaner(test_ip)\n",
        "vect_neg=t2v(tokens,feat_selection)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Classifier"
      ],
      "metadata": {
        "id": "DW4Wg_HME2Me"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5Arpd76CTSj"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of samples, Number of features\")\n",
        "vector_feat.shape"
      ],
      "metadata": {
        "id": "cwo-BnxGFhvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQkxAp1mCTSj"
      },
      "outputs": [],
      "source": [
        "svml=svm.LinearSVC()\n",
        "model = svml.fit(vector_feat,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ld1ARnnCTSj"
      },
      "outputs": [],
      "source": [
        "res = model.predict([vect_pos])\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4BWt2-ICTSj"
      },
      "outputs": [],
      "source": [
        "res = model.predict([vect_neg])\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of samples, Number of features\")\n",
        "vector.shape"
      ],
      "metadata": {
        "id": "ee93ZbxIFcNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-77F6swPCTSj"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(vector, y)\n",
        "t_model = SelectFromModel(lsvc, prefit=True)\n",
        "X_new = t_model.transform(vector)\n",
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pabNzpSeCTSk"
      },
      "outputs": [],
      "source": [
        "X_new[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVAHhh72CTSk"
      },
      "outputs": [],
      "source": [
        "svml=svm.LinearSVC()\n",
        "model = svml.fit(X_new,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_URFPyYTCTSk"
      },
      "outputs": [],
      "source": [
        "test_ip=\"I love watching netflix original movies\"\n",
        "tokens=text_cleaner(test_ip)\n",
        "vect_pos=t2v(tokens,vocabulary)\n",
        "vect_pos = t_model.transform(np.asarray([vect_pos]))\n",
        "vect_pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkLNSVc2CTSk"
      },
      "outputs": [],
      "source": [
        "res = model.predict(vect_pos)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVM-Zl5xCTSk"
      },
      "outputs": [],
      "source": [
        "test_ip=\"I hate watching netflix original movies\"\n",
        "tokens=text_cleaner(test_ip)\n",
        "vect_neg=t2v(tokens,vocabulary)\n",
        "vect_neg = t_model.transform(np.asarray([vect_neg]))\n",
        "vect_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA6hScXHCTSk"
      },
      "outputs": [],
      "source": [
        "res = model.predict(vect_neg)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nvMlF2dCTSk"
      },
      "source": [
        "# Dimensional Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DJ92hXYCTSk"
      },
      "source": [
        "### PCA based dimensional reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b_0USCvCTSl"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=200)\n",
        "X_new=pca.fit_transform(vector)\n",
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6HBUKwvCTSl"
      },
      "outputs": [],
      "source": [
        "svml=svm.LinearSVC()\n",
        "model = svml.fit(X_new,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKWN1EjVCTSl"
      },
      "outputs": [],
      "source": [
        "test_ip=\"I love watching netflix original movies\"\n",
        "tokens=text_cleaner(test_ip)\n",
        "vect_pos=t2v(tokens,vocabulary)\n",
        "vect_pos = pca.transform(np.asarray([vect_pos]))\n",
        "vect_pos.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTpeaIvECTSl"
      },
      "outputs": [],
      "source": [
        "res = model.predict(vect_pos)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDD1ZC4dCTSl"
      },
      "outputs": [],
      "source": [
        "test_ip=\"I hate watching netflix original movies\"\n",
        "tokens=text_cleaner(test_ip)\n",
        "vect_neg=t2v(tokens,vocabulary)\n",
        "vect_neg = pca.transform(np.asarray([vect_neg]))\n",
        "vect_neg.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAPQ8UjbCTSl"
      },
      "outputs": [],
      "source": [
        "res = model.predict(vect_neg)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkmrR3OoCTSm"
      },
      "source": [
        "### SVD based dimensional reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBJ292WRCTSm"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=200, n_iter=7, random_state=42)\n",
        "X_new=svd.fit_transform(vector)\n",
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAwV0uEPCTSm"
      },
      "outputs": [],
      "source": [
        "svml=svm.LinearSVC()\n",
        "model = svml.fit(X_new,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD5OCN3WCTSm"
      },
      "outputs": [],
      "source": [
        "test_ip=\"I love watching netflix original movies\"\n",
        "tokens=text_cleaner(test_ip)\n",
        "vect_pos=t2v(tokens,vocabulary)\n",
        "vect_pos = svd.transform(np.asarray([vect_pos]))\n",
        "vect_pos.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED5Dspd0CTSn"
      },
      "outputs": [],
      "source": [
        "test_ip=\"I hate watching netflix original movies\"\n",
        "tokens=text_cleaner(test_ip)\n",
        "vect_neg=t2v(tokens,vocabulary)\n",
        "vect_neg = svd.transform(np.asarray([vect_neg]))\n",
        "vect_neg.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZnca6R6CTSn"
      },
      "outputs": [],
      "source": [
        "res = model.predict(vect_pos)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piWMC4kQCTSn"
      },
      "outputs": [],
      "source": [
        "res = model.predict(vect_neg)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ],
      "metadata": {
        "id": "grrasSABH7zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim, logging, os\n",
        "from gensim import corpora\n",
        "from collections import defaultdict\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "t1 = time.time()"
      ],
      "metadata": {
        "id": "lGRc1LwEH7ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "id": "bsDKCrctIqa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMo9P44dCTSn"
      },
      "outputs": [],
      "source": [
        "texts = [[word for word in text_cleaner(line.strip().split('\\t')[0])] for line in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "id": "bzC8wNpGJ8T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "metadata": {
        "id": "tQMWlIggIYJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(\n",
        "    sentences=texts,\n",
        "    min_count=0,\n",
        "    sample=0.001,\n",
        "    seed=1,\n",
        "    workers=8,\n",
        "    min_alpha=0.0001,\n",
        "    sg=0,\n",
        "    hs=0,\n",
        "    negative=5,\n",
        "    epochs=100,\n",
        "    vector_size=50\n",
        ")"
      ],
      "metadata": {
        "id": "5gnOgTnSI7ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv['unjustified']\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "dZz7UgmIJ2ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sims = model.wv.most_similar('responsible', topn=10)\n",
        "for word, score in sims:\n",
        "    print(f\"{word}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "5CU9YUSWJKfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.wv.most_similar(positive=['healthcare', 'clinics'], negative=['death'], topn=1)\n",
        "print(result) # Expected output: [('queen', 0.7118)]"
      ],
      "metadata": {
        "id": "07dyfxqXJsnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "2ojNK2b_Nz0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = ['rt','amp','url','sir','day','title','shri','crore','time',\"a\", \"about\",\"above\", \"across\", \"after\", \"afterwards\", \"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
        "tstop = ['rt','amp','url']\n",
        "import itertools\n",
        "\n",
        "import re\n",
        "\n",
        "def text_cleaner(text):\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\" \",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z_@#]\", \" \", newString)\n",
        "    long_words=[]\n",
        "    for i in newString.split():\n",
        "        if i not in tstop:\n",
        "            long_words.append(i)\n",
        "    return long_words\n",
        "\n",
        "import emoji\n",
        "\n",
        "def remove_link(text):\n",
        "    regex = r'https?://[^\\s<>)\"‘’]+'\n",
        "    match = re.sub(regex,' ', text)\n",
        "    regex = r'urls?'\n",
        "    match = re.sub(regex,' ', match)\n",
        "    tweet=emoji.demojize(match)\n",
        "    tweet = re.sub(\"[0-9:;,.()!?…]\", \" \", tweet)\n",
        "    tweet = re.sub(\"[ ]+\", \" \", tweet)\n",
        "    return tweet.strip()"
      ],
      "metadata": {
        "id": "rbBJeBCbN6HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "fC7fKKx-N7FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
        "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in sentences]"
      ],
      "metadata": {
        "id": "mbvZzqPHOPNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 60\n",
        "window_size = 10\n",
        "min_word = 5\n",
        "down_sampling = 1e-2"
      ],
      "metadata": {
        "id": "GLI1Iqs1OYpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = FastText(\n",
        "    sentences=word_tokenized_corpus,\n",
        "    vector_size=embedding_size,\n",
        "    window=window_size,\n",
        "    min_count=min_word,\n",
        "    sample=down_sampling,\n",
        "    sg=1,\n",
        "    epochs=100\n",
        ")"
      ],
      "metadata": {
        "id": "3kJOYmHzOa4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.wv.most_similar(['responsible'], topn=10)"
      ],
      "metadata": {
        "id": "NSy3PQ6VOpaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = ft_model.wv.most_similar(positive=['healthcare', 'clinics'], negative=['death'], topn=1)\n",
        "\n",
        "print(f\"Analogy result: {result[0][0]}\")"
      ],
      "metadata": {
        "id": "N9Uv6M-GO4ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MX02cm5toQIa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}